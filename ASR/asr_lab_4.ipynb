{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW_ asr_lab_4_new_ipynb  (copy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcyxmRJGqlY"
      },
      "source": [
        "# Практика №4\n",
        "\n",
        "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbO_rrWGq7j"
      },
      "source": [
        "### Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzJyomV1JaLp",
        "outputId": "5e0067c3-b4b5-4d78-8ae7-9ef29ad5a3b3"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install sentencepiece\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.8.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TROAsHTXHWik",
        "outputId": "e8bc8d83-4a76-465e-fe0f-bb49fe96c765"
      },
      "source": [
        "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
        "\n",
        "!unzip -q lab4.zip\n",
        "!rm -rf lab4.zip sample_data\n",
        "%cd lab4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/dm/MADE-22/asr-n-tts/asr_lab4/lab4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4wcCtkIH2dn"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from utils import TextTransform\n",
        "from utils import cer\n",
        "from utils import wer\n",
        "\n",
        "import espnet\n",
        "from espnet.nets.pytorch_backend.conformer.convolution import ConvolutionModule\n",
        "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
        "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n",
        "from espnet.nets.pytorch_backend.conformer.encoder_layer import EncoderLayer as ConformerEncoderLayer\n",
        "\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnkBIr9umS07",
        "outputId": "ca7bdd00-6abc-43e9-d218-9853fce9bfe6"
      },
      "source": [
        "display(torch.__version__)\n",
        "display(espnet.__version__)\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'1.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'0.9.7'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQw5WAjWmS0_"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = (\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSGxpGS3mS1B"
      },
      "source": [
        "#-----------------------------TODO №2-----------------------------------\n",
        "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
        "#-----------------------------------------------------------------------\n",
        "class TextTransformBPE:\n",
        "    def __init__(self):\n",
        "        \"\"\" Обучение BPE модели на 4000 юнитов\"\"\"\n",
        "        self.train_data=\"train_clean_100_text_clean.txt\"\n",
        "        self.vocab_size=4000\n",
        "        train_cmd = f\"--input={self.train_data} --model_prefix=m_bpe --vocab_size={self.vocab_size} --model_type=bpe\"\n",
        "        spm.SentencePieceTrainer.train(train_cmd)        \n",
        "        self.sp_bpe = spm.SentencePieceProcessor()\n",
        "        self.sp_bpe.load('m_bpe.model')\n",
        "        \n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
        "        return self.sp_bpe.encode_as_ids(text)\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
        "        return ''.join(self.sp_bpe.decode_ids(labels))\n",
        "\n",
        "#text_transform = TextTransform()\n",
        "text_transform = TextTransformBPE()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaESUZiHJgfN"
      },
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.upper()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=4000, collapse_repeated=True):\n",
        "#def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        target_ids = list(map(int, labels[i][:label_lengths[i]].tolist()))\n",
        "        targets.append(text_transform.int_to_text(target_ids))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqoVLnrJsCV"
      },
      "source": [
        "class TransformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "         output_size=4001,\n",
        "#        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, dropout\n",
        "                ),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        gc.collect()\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p_8IjeKkqq"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, num_batches, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
        "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 500 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{: 5d}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                500. * batch_idx / data_len, loss.item(), scheduler.get_last_lr()[0]))\n",
        "            \n",
        "        batches_exceed = False\n",
        "        if num_batches:\n",
        "            batches_exceed = batch_idx * len(spectrograms) > num_batches\n",
        "        \n",
        "        del spectrograms\n",
        "        del labels\n",
        "        del input_lengths\n",
        "        del label_lengths\n",
        "        del _data\n",
        "        del output\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        if batches_exceed:\n",
        "            break\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
        "            \n",
        "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "            \n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "                \n",
        "            del spectrograms\n",
        "            del labels\n",
        "            del input_lengths\n",
        "            del label_lengths\n",
        "            del _data\n",
        "            del output\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzEbtsB1LKsh"
      },
      "source": [
        "def main(EncoderModel, learning_rate=1e-5,\n",
        "         batch_size=20, test_batch_size=7, num_batches=None, epochs=10,\n",
        "         train_url=\"train-clean-100\", test_url=\"test-clean\"\n",
        "        ):\n",
        "    \n",
        "    hparams = {\n",
        "        \"input_size\": 80,\n",
        "         \"output_size\": 4001,\n",
        "#        \"output_size\": 29,\n",
        "        \"conv2d_filters\": 32,\n",
        "        \"attention_dim\": 360,\n",
        "        \"attention_heads\": 8,\n",
        "        \"feedforward_dim\": 1024,\n",
        "        \"num_layers\": 10,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=test_batch_size,\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = EncoderModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']).to(device)\n",
        "\n",
        "    #print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "    \n",
        "    steps_per_epoch = int(len(train_loader))\n",
        "    if num_batches:\n",
        "        steps_per_epoch = num_batches // batch_size + batch_size\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "#     criterion = torch.nn.CTCLoss(blank=28, zero_infinity=False).to(device)\n",
        "    criterion = torch.nn.CTCLoss(blank=4000, zero_infinity=False).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=steps_per_epoch,\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        !date\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, num_batches, epoch)\n",
        "        !date\n",
        "        test(model, device, test_loader, criterion, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby39YVqZadd"
      },
      "source": [
        "### <b>Задание №1</b> (5 баллов):\n",
        "На данный момент практически все E2E SOTA решения использую [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExZLsUiLeXk",
        "scrolled": true,
        "outputId": "ba4bf3a6-fcc2-4e9b-a592-51345ef2487a"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 3\n",
        "test_batch_size = 3\n",
        "num_batches = 15000\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(TransformerModel,\n",
        "     learning_rate,\n",
        "     batch_size,\n",
        "     test_batch_size,\n",
        "     num_batches,\n",
        "     epochs,\n",
        "     libri_train_set,\n",
        "     libri_test_set\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Model Parameters 14284849\n",
            "Вт 11 мая 2021 19:31:33 +05\n",
            "Train Epoch: 1 [    0/28539 (0%)]\tLoss: 46.194115\tLR: 0.000040\n",
            "Train Epoch: 1 [ 1500/28539 (9%)]\tLoss: 6.922698\tLR: 0.000072\n",
            "Train Epoch: 1 [ 3000/28539 (18%)]\tLoss: 6.836174\tLR: 0.000104\n",
            "Train Epoch: 1 [ 4500/28539 (26%)]\tLoss: 6.756629\tLR: 0.000136\n",
            "Train Epoch: 1 [ 6000/28539 (35%)]\tLoss: 7.108911\tLR: 0.000168\n",
            "Train Epoch: 1 [ 7500/28539 (44%)]\tLoss: 7.201813\tLR: 0.000200\n",
            "Train Epoch: 1 [ 9000/28539 (53%)]\tLoss: 6.641447\tLR: 0.000232\n",
            "Train Epoch: 1 [ 10500/28539 (61%)]\tLoss: 6.703844\tLR: 0.000264\n",
            "Train Epoch: 1 [ 12000/28539 (70%)]\tLoss: 6.983306\tLR: 0.000296\n",
            "Train Epoch: 1 [ 13500/28539 (79%)]\tLoss: 6.659344\tLR: 0.000328\n",
            "Train Epoch: 1 [ 15000/28539 (88%)]\tLoss: 6.713657\tLR: 0.000360\n",
            "Вт 11 мая 2021 19:45:00 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 6.7421, Average CER: 0.998781 Average WER: 0.9991\n",
            "\n",
            "Вт 11 мая 2021 19:46:10 +05\n",
            "Train Epoch: 2 [    0/28539 (0%)]\tLoss: 6.663968\tLR: 0.000360\n",
            "Train Epoch: 2 [ 1500/28539 (9%)]\tLoss: 6.553470\tLR: 0.000392\n",
            "Train Epoch: 2 [ 3000/28539 (18%)]\tLoss: 6.365277\tLR: 0.000424\n",
            "Train Epoch: 2 [ 4500/28539 (26%)]\tLoss: 7.498061\tLR: 0.000456\n",
            "Train Epoch: 2 [ 6000/28539 (35%)]\tLoss: 6.629905\tLR: 0.000488\n",
            "Train Epoch: 2 [ 7500/28539 (44%)]\tLoss: 6.404394\tLR: 0.000520\n",
            "Train Epoch: 2 [ 9000/28539 (53%)]\tLoss: 5.430286\tLR: 0.000552\n",
            "Train Epoch: 2 [ 10500/28539 (61%)]\tLoss: 6.424835\tLR: 0.000584\n",
            "Train Epoch: 2 [ 12000/28539 (70%)]\tLoss: 6.287335\tLR: 0.000616\n",
            "Train Epoch: 2 [ 13500/28539 (79%)]\tLoss: 6.147027\tLR: 0.000648\n",
            "Train Epoch: 2 [ 15000/28539 (88%)]\tLoss: 5.795673\tLR: 0.000680\n",
            "Вт 11 мая 2021 19:59:21 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 5.6619, Average CER: 0.920685 Average WER: 0.9618\n",
            "\n",
            "Вт 11 мая 2021 20:00:59 +05\n",
            "Train Epoch: 3 [    0/28539 (0%)]\tLoss: 5.839756\tLR: 0.000680\n",
            "Train Epoch: 3 [ 1500/28539 (9%)]\tLoss: 6.078935\tLR: 0.000712\n",
            "Train Epoch: 3 [ 3000/28539 (18%)]\tLoss: 6.360265\tLR: 0.000744\n",
            "Train Epoch: 3 [ 4500/28539 (26%)]\tLoss: 5.601700\tLR: 0.000776\n",
            "Train Epoch: 3 [ 6000/28539 (35%)]\tLoss: 5.601739\tLR: 0.000808\n",
            "Train Epoch: 3 [ 7500/28539 (44%)]\tLoss: 6.114902\tLR: 0.000840\n",
            "Train Epoch: 3 [ 9000/28539 (53%)]\tLoss: 5.454521\tLR: 0.000872\n",
            "Train Epoch: 3 [ 10500/28539 (61%)]\tLoss: 5.025488\tLR: 0.000904\n",
            "Train Epoch: 3 [ 12000/28539 (70%)]\tLoss: 5.470346\tLR: 0.000936\n",
            "Train Epoch: 3 [ 13500/28539 (79%)]\tLoss: 5.465984\tLR: 0.000968\n",
            "Train Epoch: 3 [ 15000/28539 (88%)]\tLoss: 6.123287\tLR: 0.001000\n",
            "Вт 11 мая 2021 20:14:04 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 5.1809, Average CER: 0.860070 Average WER: 0.9065\n",
            "\n",
            "Вт 11 мая 2021 20:16:07 +05\n",
            "Train Epoch: 4 [    0/28539 (0%)]\tLoss: 5.695182\tLR: 0.001000\n",
            "Train Epoch: 4 [ 1500/28539 (9%)]\tLoss: 5.262600\tLR: 0.000986\n",
            "Train Epoch: 4 [ 3000/28539 (18%)]\tLoss: 5.715581\tLR: 0.000971\n",
            "Train Epoch: 4 [ 4500/28539 (26%)]\tLoss: 4.981832\tLR: 0.000957\n",
            "Train Epoch: 4 [ 6000/28539 (35%)]\tLoss: 5.622897\tLR: 0.000943\n",
            "Train Epoch: 4 [ 7500/28539 (44%)]\tLoss: 5.371648\tLR: 0.000929\n",
            "Train Epoch: 4 [ 9000/28539 (53%)]\tLoss: 5.447248\tLR: 0.000914\n",
            "Train Epoch: 4 [ 10500/28539 (61%)]\tLoss: 5.797428\tLR: 0.000900\n",
            "Train Epoch: 4 [ 12000/28539 (70%)]\tLoss: 5.234404\tLR: 0.000886\n",
            "Train Epoch: 4 [ 13500/28539 (79%)]\tLoss: 5.557563\tLR: 0.000872\n",
            "Train Epoch: 4 [ 15000/28539 (88%)]\tLoss: 5.398288\tLR: 0.000857\n",
            "Вт 11 мая 2021 20:29:07 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.7414, Average CER: 0.804029 Average WER: 0.8694\n",
            "\n",
            "Вт 11 мая 2021 20:31:37 +05\n",
            "Train Epoch: 5 [    0/28539 (0%)]\tLoss: 5.162831\tLR: 0.000857\n",
            "Train Epoch: 5 [ 1500/28539 (9%)]\tLoss: 5.339564\tLR: 0.000843\n",
            "Train Epoch: 5 [ 3000/28539 (18%)]\tLoss: 5.324998\tLR: 0.000829\n",
            "Train Epoch: 5 [ 4500/28539 (26%)]\tLoss: 4.760519\tLR: 0.000814\n",
            "Train Epoch: 5 [ 6000/28539 (35%)]\tLoss: 5.466664\tLR: 0.000800\n",
            "Train Epoch: 5 [ 7500/28539 (44%)]\tLoss: 4.952493\tLR: 0.000786\n",
            "Train Epoch: 5 [ 9000/28539 (53%)]\tLoss: 5.669567\tLR: 0.000772\n",
            "Train Epoch: 5 [ 10500/28539 (61%)]\tLoss: 5.029718\tLR: 0.000757\n",
            "Train Epoch: 5 [ 12000/28539 (70%)]\tLoss: 4.636837\tLR: 0.000743\n",
            "Train Epoch: 5 [ 13500/28539 (79%)]\tLoss: 5.049544\tLR: 0.000729\n",
            "Train Epoch: 5 [ 15000/28539 (88%)]\tLoss: 4.538033\tLR: 0.000714\n",
            "Вт 11 мая 2021 20:44:38 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.5124, Average CER: 0.825354 Average WER: 0.8666\n",
            "\n",
            "Вт 11 мая 2021 20:46:53 +05\n",
            "Train Epoch: 6 [    0/28539 (0%)]\tLoss: 4.387857\tLR: 0.000714\n",
            "Train Epoch: 6 [ 1500/28539 (9%)]\tLoss: 4.798656\tLR: 0.000700\n",
            "Train Epoch: 6 [ 3000/28539 (18%)]\tLoss: 4.719950\tLR: 0.000686\n",
            "Train Epoch: 6 [ 4500/28539 (26%)]\tLoss: 4.635252\tLR: 0.000672\n",
            "Train Epoch: 6 [ 6000/28539 (35%)]\tLoss: 4.743164\tLR: 0.000657\n",
            "Train Epoch: 6 [ 7500/28539 (44%)]\tLoss: 4.747722\tLR: 0.000643\n",
            "Train Epoch: 6 [ 9000/28539 (53%)]\tLoss: 4.712017\tLR: 0.000629\n",
            "Train Epoch: 6 [ 10500/28539 (61%)]\tLoss: 4.618631\tLR: 0.000614\n",
            "Train Epoch: 6 [ 12000/28539 (70%)]\tLoss: 4.695325\tLR: 0.000600\n",
            "Train Epoch: 6 [ 13500/28539 (79%)]\tLoss: 4.504666\tLR: 0.000586\n",
            "Train Epoch: 6 [ 15000/28539 (88%)]\tLoss: 4.743742\tLR: 0.000572\n",
            "Вт 11 мая 2021 20:59:56 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.3183, Average CER: 0.762821 Average WER: 0.8340\n",
            "\n",
            "Вт 11 мая 2021 21:02:39 +05\n",
            "Train Epoch: 7 [    0/28539 (0%)]\tLoss: 4.491344\tLR: 0.000572\n",
            "Train Epoch: 7 [ 1500/28539 (9%)]\tLoss: 4.281998\tLR: 0.000557\n",
            "Train Epoch: 7 [ 3000/28539 (18%)]\tLoss: 4.737076\tLR: 0.000543\n",
            "Train Epoch: 7 [ 4500/28539 (26%)]\tLoss: 4.269875\tLR: 0.000529\n",
            "Train Epoch: 7 [ 6000/28539 (35%)]\tLoss: 4.916085\tLR: 0.000514\n",
            "Train Epoch: 7 [ 7500/28539 (44%)]\tLoss: 4.284009\tLR: 0.000500\n",
            "Train Epoch: 7 [ 9000/28539 (53%)]\tLoss: 4.205467\tLR: 0.000486\n",
            "Train Epoch: 7 [ 10500/28539 (61%)]\tLoss: 4.766701\tLR: 0.000472\n",
            "Train Epoch: 7 [ 12000/28539 (70%)]\tLoss: 4.805670\tLR: 0.000457\n",
            "Train Epoch: 7 [ 13500/28539 (79%)]\tLoss: 4.271582\tLR: 0.000443\n",
            "Train Epoch: 7 [ 15000/28539 (88%)]\tLoss: 4.818565\tLR: 0.000429\n",
            "Вт 11 мая 2021 21:15:48 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.1643, Average CER: 0.725437 Average WER: 0.8065\n",
            "\n",
            "Вт 11 мая 2021 21:18:47 +05\n",
            "Train Epoch: 8 [    0/28539 (0%)]\tLoss: 4.381749\tLR: 0.000429\n",
            "Train Epoch: 8 [ 1500/28539 (9%)]\tLoss: 4.027906\tLR: 0.000414\n",
            "Train Epoch: 8 [ 3000/28539 (18%)]\tLoss: 4.603005\tLR: 0.000400\n",
            "Train Epoch: 8 [ 4500/28539 (26%)]\tLoss: 4.200009\tLR: 0.000386\n",
            "Train Epoch: 8 [ 6000/28539 (35%)]\tLoss: 4.296649\tLR: 0.000372\n",
            "Train Epoch: 8 [ 7500/28539 (44%)]\tLoss: 4.190413\tLR: 0.000357\n",
            "Train Epoch: 8 [ 9000/28539 (53%)]\tLoss: 3.794005\tLR: 0.000343\n",
            "Train Epoch: 8 [ 10500/28539 (61%)]\tLoss: 3.901192\tLR: 0.000329\n",
            "Train Epoch: 8 [ 12000/28539 (70%)]\tLoss: 4.355021\tLR: 0.000314\n",
            "Train Epoch: 8 [ 13500/28539 (79%)]\tLoss: 4.221518\tLR: 0.000300\n",
            "Train Epoch: 8 [ 15000/28539 (88%)]\tLoss: 4.227710\tLR: 0.000286\n",
            "Вт 11 мая 2021 21:31:57 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.0473, Average CER: 0.732342 Average WER: 0.8032\n",
            "\n",
            "Вт 11 мая 2021 21:34:51 +05\n",
            "Train Epoch: 9 [    0/28539 (0%)]\tLoss: 4.076500\tLR: 0.000286\n",
            "Train Epoch: 9 [ 1500/28539 (9%)]\tLoss: 3.928429\tLR: 0.000272\n",
            "Train Epoch: 9 [ 3000/28539 (18%)]\tLoss: 4.396094\tLR: 0.000257\n",
            "Train Epoch: 9 [ 4500/28539 (26%)]\tLoss: 4.687136\tLR: 0.000243\n",
            "Train Epoch: 9 [ 6000/28539 (35%)]\tLoss: 4.173903\tLR: 0.000229\n",
            "Train Epoch: 9 [ 7500/28539 (44%)]\tLoss: 4.427769\tLR: 0.000215\n",
            "Train Epoch: 9 [ 9000/28539 (53%)]\tLoss: 3.926017\tLR: 0.000200\n",
            "Train Epoch: 9 [ 10500/28539 (61%)]\tLoss: 4.420616\tLR: 0.000186\n",
            "Train Epoch: 9 [ 12000/28539 (70%)]\tLoss: 4.160180\tLR: 0.000172\n",
            "Train Epoch: 9 [ 13500/28539 (79%)]\tLoss: 4.103481\tLR: 0.000157\n",
            "Train Epoch: 9 [ 15000/28539 (88%)]\tLoss: 4.312164\tLR: 0.000143\n",
            "Вт 11 мая 2021 21:47:59 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 3.9358, Average CER: 0.699282 Average WER: 0.7790\n",
            "\n",
            "Вт 11 мая 2021 21:51:07 +05\n",
            "Train Epoch: 10 [    0/28539 (0%)]\tLoss: 4.396142\tLR: 0.000143\n",
            "Train Epoch: 10 [ 1500/28539 (9%)]\tLoss: 4.297513\tLR: 0.000129\n",
            "Train Epoch: 10 [ 3000/28539 (18%)]\tLoss: 4.051303\tLR: 0.000115\n",
            "Train Epoch: 10 [ 4500/28539 (26%)]\tLoss: 3.858354\tLR: 0.000100\n",
            "Train Epoch: 10 [ 6000/28539 (35%)]\tLoss: 4.079469\tLR: 0.000086\n",
            "Train Epoch: 10 [ 7500/28539 (44%)]\tLoss: 4.063595\tLR: 0.000072\n",
            "Train Epoch: 10 [ 9000/28539 (53%)]\tLoss: 4.293652\tLR: 0.000057\n",
            "Train Epoch: 10 [ 10500/28539 (61%)]\tLoss: 4.287395\tLR: 0.000043\n",
            "Train Epoch: 10 [ 12000/28539 (70%)]\tLoss: 4.149110\tLR: 0.000029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 10 [ 13500/28539 (79%)]\tLoss: 4.122942\tLR: 0.000015\n",
            "Train Epoch: 10 [ 15000/28539 (88%)]\tLoss: 4.422119\tLR: 0.000000\n",
            "Вт 11 мая 2021 22:04:13 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 3.8841, Average CER: 0.691785 Average WER: 0.7755\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV48Q7HqZsAD"
      },
      "source": [
        "### <b>Задание №2</b> (5 баллов):\n",
        "Импровизация по улучшению качества распознавания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9s9ULmhmS1O"
      },
      "source": [
        "class ConformerModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=4001,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "        cnn_module_kernel=7,\n",
        "        macaron_style=False,\n",
        "    ):\n",
        "        super(ConformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        \n",
        "        # \"swish\" activation\n",
        "        activation = lambda x: x * torch.sigmoid(x)\n",
        "        \n",
        "        # feed-forward module definition\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        \n",
        "        # self-attention module definition\n",
        "        encoder_selfattn_layer = MultiHeadedAttention\n",
        "        encoder_selfattn_layer_args = (attention_heads, attention_dim, dropout)\n",
        "        \n",
        "        # convolution module definition\n",
        "        convolution_layer = ConvolutionModule\n",
        "        convolution_layer_args = (attention_dim, cnn_module_kernel, activation)\n",
        "        \n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: ConformerEncoderLayer(\n",
        "                attention_dim,\n",
        "                encoder_selfattn_layer(*encoder_selfattn_layer_args),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                positionwise_layer(*positionwise_layer_args) if macaron_style else None,\n",
        "                convolution_layer(*convolution_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        \n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x[0])\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q13aYBklmS1P",
        "outputId": "dab1ff41-9ab1-46ff-e662-43335e444363"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 3\n",
        "test_batch_size = 3\n",
        "num_batches=12000\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(ConformerModel,\n",
        "     learning_rate,\n",
        "     batch_size,\n",
        "     test_batch_size,\n",
        "     num_batches,\n",
        "     epochs,\n",
        "     libri_train_set,\n",
        "     libri_test_set\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Model Parameters 18234049\n",
            "Вт 11 мая 2021 22:40:05 +05\n",
            "Train Epoch: 1 [    0/28539 (0%)]\tLoss: 50.231560\tLR: 0.000040\n",
            "Train Epoch: 1 [ 1500/28539 (9%)]\tLoss: 7.076896\tLR: 0.000080\n",
            "Train Epoch: 1 [ 3000/28539 (18%)]\tLoss: 6.978543\tLR: 0.000120\n",
            "Train Epoch: 1 [ 4500/28539 (26%)]\tLoss: 6.858655\tLR: 0.000160\n",
            "Train Epoch: 1 [ 6000/28539 (35%)]\tLoss: 6.755114\tLR: 0.000200\n",
            "Train Epoch: 1 [ 7500/28539 (44%)]\tLoss: 7.321023\tLR: 0.000240\n",
            "Train Epoch: 1 [ 9000/28539 (53%)]\tLoss: 6.711530\tLR: 0.000280\n",
            "Train Epoch: 1 [ 10500/28539 (61%)]\tLoss: 6.464132\tLR: 0.000320\n",
            "Train Epoch: 1 [ 12000/28539 (70%)]\tLoss: 6.489318\tLR: 0.000360\n",
            "Вт 11 мая 2021 22:52:47 +05\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 7.5704, Average CER: 1.000000 Average WER: 0.9997\n",
            "\n",
            "Вт 11 мая 2021 22:53:51 +05\n",
            "Train Epoch: 2 [    0/28539 (0%)]\tLoss: 6.645380\tLR: 0.000360\n",
            "Train Epoch: 2 [ 1500/28539 (9%)]\tLoss: 6.822620\tLR: 0.000400\n",
            "Train Epoch: 2 [ 3000/28539 (18%)]\tLoss: 6.572341\tLR: 0.000440\n",
            "Train Epoch: 2 [ 4500/28539 (26%)]\tLoss: 6.628571\tLR: 0.000480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 3.82 GiB total capacity; 2.48 GiB already allocated; 32.19 MiB free; 2.60 GiB reserved in total by PyTorch)",
          "traceback": [
            "\u001b[0;31m-----------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5bd1ce0c0a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m main(ConformerModel,\n\u001b[0m\u001b[1;32m     10\u001b[0m      \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-42bba03a0c87>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(EncoderModel, learning_rate, batch_size, test_batch_size, num_batches, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-becc4eb6351b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, num_batches, epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/asr-n-tts/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/asr-n-tts/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 3.82 GiB total capacity; 2.48 GiB already allocated; 32.19 MiB free; 2.60 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MAMGsBfmS1Q"
      },
      "source": [
        "__Замечания__:\n",
        "- модель тяжелая, GPU колаб не дает, обучил, насколько хватило ноута\n",
        "- batch_size=3 для ConformerModel явно маловато, в процессе экпериментов, модель хорошо себя показывала на 20-30% тренировочных данных с обучением в 2 эпохи\n",
        "- по дальнейшей оптимизации обеих моделей, первое, что даст хороший прирост - это увеличить batch_size\n",
        "- для TransformerModel пробовал несколько вариентов lr_scheduler (StepLR, CyclicLR, OneCycleLR, ReduceLROnPlateau), OneCycleLR показал меньший лосс, оставил его\n",
        "\n",
        "Студент: Черников Дмитрий, MADE-ML-22"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxYpBfBimS1Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPh8jB1dmS1S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}